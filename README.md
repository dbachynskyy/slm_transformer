# slm_transformer
Transformer implementation based on "Attention is all you need", inspired by @karpathy
